{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations with IBM\n",
    "\n",
    "In this notebook, you will be putting your recommendation skills to use on real data from the IBM Watson Studio platform. \n",
    "\n",
    "\n",
    "You may either submit your notebook through the workspace here, or you may work from your local machine and submit through the next page.  Either way assure that your code passes the project [RUBRIC](https://review.udacity.com/#!/rubrics/2322/view).  **Please save regularly.**\n",
    "\n",
    "By following the table of contents, you will build out a number of different methods for making recommendations that can be used for different situations. \n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "I. [Exploratory Data Analysis](#Exploratory-Data-Analysis)<br>\n",
    "II. [Rank Based Recommendations](#Rank)<br>\n",
    "III. [User-User Based Collaborative Filtering](#User-User)<br>\n",
    "IV. [Content Based Recommendations (EXTRA - NOT REQUIRED)](#Content-Recs)<br>\n",
    "V. [Matrix Factorization](#Matrix-Fact)<br>\n",
    "VI. [Extras & Concluding](#conclusions)\n",
    "\n",
    "At the end of the notebook, you will find directions for how to submit your work.  Let's get started by importing the necessary libraries and reading in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1430.0</td>\n",
       "      <td>using pixiedust for fast, flexible, and easier...</td>\n",
       "      <td>ef5f11f77ba020cd36e1105a00ab868bbdbf7fe7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1314.0</td>\n",
       "      <td>healthcare python streaming application demo</td>\n",
       "      <td>083cbdfa93c8444beaa4c5f5e0f5f9198e4f9e0b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1429.0</td>\n",
       "      <td>use deep learning for image classification</td>\n",
       "      <td>b96a4f2e92d8572034b1e9b28f9ac673765cd074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1338.0</td>\n",
       "      <td>ml optimization using cognitive assistant</td>\n",
       "      <td>06485706b34a5c9bf2a0ecdac41daf7e7654ceb7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1276.0</td>\n",
       "      <td>deploy your python model as a restful api</td>\n",
       "      <td>f01220c46fc92c6e6b161b1849de11faacd7ccb2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                              title  \\\n",
       "0      1430.0  using pixiedust for fast, flexible, and easier...   \n",
       "1      1314.0       healthcare python streaming application demo   \n",
       "2      1429.0         use deep learning for image classification   \n",
       "3      1338.0          ml optimization using cognitive assistant   \n",
       "4      1276.0          deploy your python model as a restful api   \n",
       "\n",
       "                                      email  \n",
       "0  ef5f11f77ba020cd36e1105a00ab868bbdbf7fe7  \n",
       "1  083cbdfa93c8444beaa4c5f5e0f5f9198e4f9e0b  \n",
       "2  b96a4f2e92d8572034b1e9b28f9ac673765cd074  \n",
       "3  06485706b34a5c9bf2a0ecdac41daf7e7654ceb7  \n",
       "4  f01220c46fc92c6e6b161b1849de11faacd7ccb2  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import project_tests as t\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('data/user-item-interactions.csv')\n",
    "df_content = pd.read_csv('data/articles_community.csv')\n",
    "del df['Unnamed: 0']\n",
    "del df_content['Unnamed: 0']\n",
    "\n",
    "# Show df to get an idea of the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_body</th>\n",
       "      <th>doc_description</th>\n",
       "      <th>doc_full_name</th>\n",
       "      <th>doc_status</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Skip navigation Sign in SearchLoading...\\r\\n\\r...</td>\n",
       "      <td>Detect bad readings in real time using Python ...</td>\n",
       "      <td>Detect Malfunctioning IoT Sensors with Streami...</td>\n",
       "      <td>Live</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No Free Hunch Navigation * kaggle.com\\r\\n\\r\\n ...</td>\n",
       "      <td>See the forest, see the trees. Here lies the c...</td>\n",
       "      <td>Communicating data science: A guide to present...</td>\n",
       "      <td>Live</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>☰ * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...</td>\n",
       "      <td>Here’s this week’s news in Data Science and Bi...</td>\n",
       "      <td>This Week in Data Science (April 18, 2017)</td>\n",
       "      <td>Live</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DATALAYER: HIGH THROUGHPUT, LOW LATENCY AT SCA...</td>\n",
       "      <td>Learn how distributed DBs solve the problem of...</td>\n",
       "      <td>DataLayer Conference: Boost the performance of...</td>\n",
       "      <td>Live</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Skip navigation Sign in SearchLoading...\\r\\n\\r...</td>\n",
       "      <td>This video demonstrates the power of IBM DataS...</td>\n",
       "      <td>Analyze NY Restaurant data using Spark in DSX</td>\n",
       "      <td>Live</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            doc_body  \\\n",
       "0  Skip navigation Sign in SearchLoading...\\r\\n\\r...   \n",
       "1  No Free Hunch Navigation * kaggle.com\\r\\n\\r\\n ...   \n",
       "2  ☰ * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...   \n",
       "3  DATALAYER: HIGH THROUGHPUT, LOW LATENCY AT SCA...   \n",
       "4  Skip navigation Sign in SearchLoading...\\r\\n\\r...   \n",
       "\n",
       "                                     doc_description  \\\n",
       "0  Detect bad readings in real time using Python ...   \n",
       "1  See the forest, see the trees. Here lies the c...   \n",
       "2  Here’s this week’s news in Data Science and Bi...   \n",
       "3  Learn how distributed DBs solve the problem of...   \n",
       "4  This video demonstrates the power of IBM DataS...   \n",
       "\n",
       "                                       doc_full_name doc_status  article_id  \n",
       "0  Detect Malfunctioning IoT Sensors with Streami...       Live           0  \n",
       "1  Communicating data science: A guide to present...       Live           1  \n",
       "2         This Week in Data Science (April 18, 2017)       Live           2  \n",
       "3  DataLayer Conference: Boost the performance of...       Live           3  \n",
       "4      Analyze NY Restaurant data using Spark in DSX       Live           4  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show df_content to get an idea of the data\n",
    "df_content.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"Exploratory-Data-Analysis\">Part I : Exploratory Data Analysis</a>\n",
    "\n",
    "Use the dictionary and cells below to provide some insight into the descriptive statistics of the data.\n",
    "\n",
    "`1.` What is the distribution of how many articles a user interacts with in the dataset?  Provide a visual and descriptive statistics to assist with giving a look at the number of times each user interacts with an article.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45993, 3), (1056, 5))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape, df_content.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5148.000000\n",
       "mean        8.930847\n",
       "std        16.802267\n",
       "min         1.000000\n",
       "25%         1.000000\n",
       "50%         3.000000\n",
       "75%         9.000000\n",
       "max       364.000000\n",
       "Name: article_id, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usr_int = df.groupby('email').count()['article_id'].describe()\n",
    "usr_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f1887066b38>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f18ada175f8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.groupby('email').count()['article_id'].hist(bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the median and maximum number of user_article interactios below\n",
    "\n",
    "median_val = 3 # 50% of individuals interact with ____ number of articles or fewer.\n",
    "max_views_by_user = 364 # The maximum number of user-article interactions by any 1 user is ______."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Explore and remove duplicate articles from the **df_content** dataframe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "doc_body           1036\n",
       "doc_description    1022\n",
       "doc_full_name      1051\n",
       "doc_status            1\n",
       "article_id         1051\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find and explore duplicate articles\n",
    "df_content.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1056 number of lines in the data, and 1051 unique articles.\n"
     ]
    }
   ],
   "source": [
    "# Find and explore duplicate articles\n",
    "print('There are {} number of lines in the data, and {} unique articles.'.format(df_content.shape[0], \n",
    "                                                                                 df_content['article_id'].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any rows that have the same article_id - only keep the first\n",
    "df_content = df_content.drop_duplicates(subset='article_id', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "714"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['article_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1051"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_content['article_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5148"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['email'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45993"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Use the cells below to find:\n",
    "\n",
    "**a.** The number of unique articles that have an interaction with a user.  \n",
    "**b.** The number of unique articles in the dataset (whether they have any interactions or not).<br>\n",
    "**c.** The number of unique users in the dataset. (excluding null values) <br>\n",
    "**d.** The number of user-article interactions in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_articles = 714 # The number of unique articles that have at least one interaction\n",
    "total_articles = 1051 # The number of unique articles on the IBM platform\n",
    "unique_users = 5148 # The number of unique users\n",
    "user_article_interactions = 45993 # The number of user-article interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` Use the cells below to find the most viewed **article_id**, as well as how often it was viewed.  After talking to the company leaders, the `email_mapper` function was deemed a reasonable way to map users to ids.  There were a small number of null values, and it was found that all of these null values likely belonged to a single user (which is how they are stored using the function below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>email</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1429.0</th>\n",
       "      <td>937</td>\n",
       "      <td>937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330.0</th>\n",
       "      <td>927</td>\n",
       "      <td>927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431.0</th>\n",
       "      <td>671</td>\n",
       "      <td>671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427.0</th>\n",
       "      <td>643</td>\n",
       "      <td>643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1364.0</th>\n",
       "      <td>627</td>\n",
       "      <td>627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            title  email\n",
       "article_id              \n",
       "1429.0        937    937\n",
       "1330.0        927    927\n",
       "1431.0        671    671\n",
       "1427.0        643    643\n",
       "1364.0        627    627"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(by='article_id').count().sort_values(by='email', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_viewed_article_id = '1429.0' # The most viewed article in the dataset as a string with one value following the decimal \n",
    "max_views = 937 # The most viewed article in the dataset was viewed how many times?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1430.0</td>\n",
       "      <td>using pixiedust for fast, flexible, and easier...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1314.0</td>\n",
       "      <td>healthcare python streaming application demo</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1429.0</td>\n",
       "      <td>use deep learning for image classification</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1338.0</td>\n",
       "      <td>ml optimization using cognitive assistant</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1276.0</td>\n",
       "      <td>deploy your python model as a restful api</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                              title  user_id\n",
       "0      1430.0  using pixiedust for fast, flexible, and easier...        1\n",
       "1      1314.0       healthcare python streaming application demo        2\n",
       "2      1429.0         use deep learning for image classification        3\n",
       "3      1338.0          ml optimization using cognitive assistant        4\n",
       "4      1276.0          deploy your python model as a restful api        5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## No need to change the code here - this will be helpful for later parts of the notebook\n",
    "# Run this cell to map the user email to a user_id column and remove the email column\n",
    "\n",
    "def email_mapper():\n",
    "    coded_dict = dict()\n",
    "    cter = 1\n",
    "    email_encoded = []\n",
    "    \n",
    "    for val in df['email']:\n",
    "        if val not in coded_dict:\n",
    "            coded_dict[val] = cter\n",
    "            cter+=1\n",
    "        \n",
    "        email_encoded.append(coded_dict[val])\n",
    "    return email_encoded\n",
    "\n",
    "email_encoded = email_mapper()\n",
    "del df['email']\n",
    "df['user_id'] = email_encoded\n",
    "\n",
    "# show header\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like you have everything right here! Nice job!\n"
     ]
    }
   ],
   "source": [
    "## If you stored all your results in the variable names above, \n",
    "## you shouldn't need to change anything in this cell\n",
    "\n",
    "sol_1_dict = {\n",
    "    '`50% of individuals have _____ or fewer interactions.`': median_val,\n",
    "    '`The total number of user-article interactions in the dataset is ______.`': user_article_interactions,\n",
    "    '`The maximum number of user-article interactions by any 1 user is ______.`': max_views_by_user,\n",
    "    '`The most viewed article in the dataset was viewed _____ times.`': max_views,\n",
    "    '`The article_id of the most viewed article is ______.`': most_viewed_article_id,\n",
    "    '`The number of unique articles that have at least 1 rating ______.`': unique_articles,\n",
    "    '`The number of unique users in the dataset is ______`': unique_users,\n",
    "    '`The number of unique articles on the IBM platform`': total_articles\n",
    "}\n",
    "\n",
    "# Test your dictionary against the solution\n",
    "t.sol_1_test(sol_1_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"Rank\">Part II: Rank-Based Recommendations</a>\n",
    "\n",
    "Unlike in the earlier lessons, we don't actually have ratings for whether a user liked an article or not.  We only know that a user has interacted with an article.  In these cases, the popularity of an article can really only be based on how often an article was interacted with.\n",
    "\n",
    "`1.` Fill in the function below to return the **n** top articles ordered with most interactions as the top. Test your function using the tests below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>use deep learning for image classification</th>\n",
       "      <td>937</td>\n",
       "      <td>937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insights from new york car accident reports</th>\n",
       "      <td>927</td>\n",
       "      <td>927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visualize car data with brunel</th>\n",
       "      <td>671</td>\n",
       "      <td>671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use xgboost, scikit-learn &amp; ibm watson machine learning apis</th>\n",
       "      <td>643</td>\n",
       "      <td>643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predicting churn with the spss random tree algorithm</th>\n",
       "      <td>627</td>\n",
       "      <td>627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>healthcare python streaming application demo</th>\n",
       "      <td>614</td>\n",
       "      <td>614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    article_id  user_id\n",
       "title                                                                  \n",
       "use deep learning for image classification                 937      937\n",
       "insights from new york car accident reports                927      927\n",
       "visualize car data with brunel                             671      671\n",
       "use xgboost, scikit-learn & ibm watson machine ...         643      643\n",
       "predicting churn with the spss random tree algo...         627      627\n",
       "healthcare python streaming application demo               614      614"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(by='title').count().sort_values(by='user_id', ascending=False).head(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1429.0</td>\n",
       "      <td>937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1330.0</td>\n",
       "      <td>927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1431.0</td>\n",
       "      <td>671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1427.0</td>\n",
       "      <td>643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1364.0</td>\n",
       "      <td>627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  article_id\n",
       "0  1429.0         937\n",
       "1  1330.0         927\n",
       "2  1431.0         671\n",
       "3  1427.0         643\n",
       "4  1364.0         627"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['article_id'].value_counts().reset_index().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_articles(n, df=df):\n",
    "    '''\n",
    "    INPUT:\n",
    "    n - (int) the number of top articles to return\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook \n",
    "    \n",
    "    OUTPUT:\n",
    "    top_articles - (list) A list of the top 'n' article titles \n",
    "    \n",
    "    '''\n",
    "    # Your code here\n",
    "    top_articles = list(df['title'].value_counts().reset_index().head(n)['index'])\n",
    "    \n",
    "    return top_articles # Return the top article titles from df (not df_content)\n",
    "\n",
    "def get_top_article_ids(n, df=df):\n",
    "    '''\n",
    "    INPUT:\n",
    "    n - (int) the number of top articles to return\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook \n",
    "    \n",
    "    OUTPUT:\n",
    "    top_articles - (list) A list of the top 'n' article titles \n",
    "    \n",
    "    '''\n",
    "    # Your code here\n",
    "    top_articles = list(df['article_id'].value_counts().reset_index().head(n)['index'])\n",
    " \n",
    "    return top_articles # Return the top article ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['use deep learning for image classification', 'insights from new york car accident reports', 'visualize car data with brunel', 'use xgboost, scikit-learn & ibm watson machine learning apis', 'predicting churn with the spss random tree algorithm', 'healthcare python streaming application demo', 'finding optimal locations of new store using decision optimization', 'apache spark lab, part 1: basic concepts', 'analyze energy consumption in buildings', 'gosales transactions for logistic regression model']\n",
      "[1429.0, 1330.0, 1431.0, 1427.0, 1364.0, 1314.0, 1293.0, 1170.0, 1162.0, 1304.0]\n"
     ]
    }
   ],
   "source": [
    "print(get_top_articles(10))\n",
    "print(get_top_article_ids(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your top_5 looks like the solution list! Nice job.\n",
      "Your top_10 looks like the solution list! Nice job.\n",
      "Your top_20 looks like the solution list! Nice job.\n"
     ]
    }
   ],
   "source": [
    "# Test your function by returning the top 5, 10, and 20 articles\n",
    "top_5 = get_top_articles(5)\n",
    "top_10 = get_top_articles(10)\n",
    "top_20 = get_top_articles(20)\n",
    "\n",
    "# Test each of your three lists from above\n",
    "t.sol_2_test(get_top_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"User-User\">Part III: User-User Based Collaborative Filtering</a>\n",
    "\n",
    "\n",
    "`1.` Use the function below to reformat the **df** dataframe to be shaped with users as the rows and articles as the columns.  \n",
    "\n",
    "* Each **user** should only appear in each **row** once.\n",
    "\n",
    "\n",
    "* Each **article** should only show up in one **column**.  \n",
    "\n",
    "\n",
    "* **If a user has interacted with an article, then place a 1 where the user-row meets for that article-column**.  It does not matter how many times a user has interacted with the article, all entries where a user has interacted with an article should be a 1.  \n",
    "\n",
    "\n",
    "* **If a user has not interacted with an item, then place a zero where the user-row meets for that article-column**. \n",
    "\n",
    "Use the tests to make sure the basic structure of your matrix matches what is expected by the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the user-article matrix with 1's and 0's\n",
    "\n",
    "def create_user_item_matrix(df):\n",
    "    '''\n",
    "    INPUT:\n",
    "    df - pandas dataframe with article_id, title, user_id columns\n",
    "    \n",
    "    OUTPUT:\n",
    "    user_item - user item matrix \n",
    "    \n",
    "    Description:\n",
    "    Return a matrix with user ids as rows and article ids on the columns with 1 values where a user interacted with \n",
    "    an article and a 0 otherwise\n",
    "    '''\n",
    "    # Fill in the function here\n",
    "    #user_item = df.groupby(by=['user_id', 'article_id']).agg(lambda x: 1).unstack().fillna(0)\n",
    "    user_item = df.groupby('user_id')['article_id'].value_counts().unstack().fillna(0)\n",
    "    user_item[user_item > 1] = 1\n",
    "    \n",
    "    return user_item # return the user_item matrix \n",
    "\n",
    "user_item = create_user_item_matrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have passed our quick tests!  Please proceed!\n"
     ]
    }
   ],
   "source": [
    "## Tests: You should just need to run this cell.  Don't change the code.\n",
    "assert user_item.shape[0] == 5149, \"Oops!  The number of users in the user-article matrix doesn't look right.\"\n",
    "assert user_item.shape[1] == 714, \"Oops!  The number of articles in the user-article matrix doesn't look right.\"\n",
    "assert user_item.sum(axis=1)[1] == 36, \"Oops!  The number of articles seen by user 1 doesn't look right.\"\n",
    "print(\"You have passed our quick tests!  Please proceed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Complete the function below which should take a user_id and provide an ordered list of the most similar users to that user (from most similar to least similar).  The returned result should not contain the provided user_id, as we know that each user is similar to him/herself. Because the results for each user here are binary, it (perhaps) makes sense to compute similarity as the dot product of two users. \n",
    "\n",
    "Use the tests to test your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_users(user_id, user_item=user_item):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user_id\n",
    "    user_item - (pandas dataframe) matrix of users by articles: \n",
    "                1's when a user has interacted with an article, 0 otherwise\n",
    "    \n",
    "    OUTPUT:\n",
    "    similar_users - (list) an ordered list where the closest users (largest dot product users)\n",
    "                    are listed first\n",
    "    \n",
    "    Description:\n",
    "    Computes the similarity of every pair of users based on the dot product\n",
    "    Returns an ordered\n",
    "    \n",
    "    '''\n",
    "    # compute similarity of each user to the provided user\n",
    "    similarity = {}\n",
    "    for user in user_item.index:\n",
    "        similarity[user] = np.dot(user_item.loc[user_id, :], user_item.loc[user, :])\n",
    "\n",
    "    # sort by similarity\n",
    "    sorted_similarity = sorted(similarity.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "    # create list of just the ids\n",
    "    most_similar_users = [key for (key, value) in sorted_similarity]\n",
    "   \n",
    "    # remove the own user's id\n",
    "    most_similar_users.remove(user_id)\n",
    "       \n",
    "    return most_similar_users # return a list of the users in order from most to least similar\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 most similar users to user 1 are: [3933, 23, 3782, 203, 4459, 131, 3870, 46, 4201, 49]\n",
      "The 5 most similar users to user 3933 are: [1, 23, 3782, 203, 4459]\n",
      "The 3 most similar users to user 46 are: [4201, 23, 3782]\n"
     ]
    }
   ],
   "source": [
    "# Do a spot check of your function\n",
    "print(\"The 10 most similar users to user 1 are: {}\".format(find_similar_users(1)[:10]))\n",
    "print(\"The 5 most similar users to user 3933 are: {}\".format(find_similar_users(3933)[:5]))\n",
    "print(\"The 3 most similar users to user 46 are: {}\".format(find_similar_users(46)[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Now that you have a function that provides the most similar users to each user, you will want to use these users to find articles you can recommend.  Complete the functions below to return the articles you would recommend to each user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_names(article_ids, df=df):\n",
    "    '''\n",
    "    INPUT:\n",
    "    article_ids - (list) a list of article ids\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook\n",
    "    \n",
    "    OUTPUT:\n",
    "    article_names - (list) a list of article names associated with the list of article ids \n",
    "                    (this is identified by the title column)\n",
    "    '''\n",
    "    # Your code here\n",
    "    article_names = df[df['article_id'].isin(article_ids)]['title']\n",
    "    \n",
    "    article_names = list(set(article_names))\n",
    "    \n",
    "    return article_names # Return the article names associated with list of article ids\n",
    "\n",
    "\n",
    "def get_user_articles(user_id, user_item=user_item):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user id\n",
    "    user_item - (pandas dataframe) matrix of users by articles: \n",
    "                1's when a user has interacted with an article, 0 otherwise\n",
    "    \n",
    "    OUTPUT:\n",
    "    article_ids - (list) a list of the article ids seen by the user\n",
    "    article_names - (list) a list of article names associated with the list of article ids \n",
    "                    (this is identified by the doc_full_name column in df_content)\n",
    "    \n",
    "    Description:\n",
    "    Provides a list of the article_ids and article titles that have been seen by a user\n",
    "    '''\n",
    "    # Your code here\n",
    "    article_col_ids = user_item.loc[user_id]\n",
    "    article_ids = [str(article_id) for article_id in article_col_ids[article_col_ids == 1].index]\n",
    "    \n",
    "    article_names = get_article_names(article_ids)\n",
    "    \n",
    "    return article_ids, article_names # return the ids and names\n",
    "\n",
    "\n",
    "def user_user_recs(user_id, m=10):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user id\n",
    "    m - (int) the number of recommendations you want for the user\n",
    "    \n",
    "    OUTPUT:\n",
    "    recs - (list) a list of recommendations for the user\n",
    "    \n",
    "    Description:\n",
    "    Loops through the users based on closeness to the input user_id\n",
    "    For each user - finds articles the user hasn't seen before and provides them as recs\n",
    "    Does this until m recommendations are found\n",
    "    \n",
    "    Notes:\n",
    "    Users who are the same closeness are chosen arbitrarily as the 'next' user\n",
    "    \n",
    "    For the user where the number of recommended articles starts below m \n",
    "    and ends exceeding m, the last items are chosen arbitrarily\n",
    "    \n",
    "    '''\n",
    "    # Your code here\n",
    "    most_similar_users = find_similar_users(user_id)\n",
    "    \n",
    "    recs = []\n",
    "    for similar_user in most_similar_users:\n",
    "        article_ids = user_item.loc[similar_user]\n",
    "        recs.extend(str(aid) for aid in article_ids[article_ids == 1].index)\n",
    "    \n",
    "    return recs # return your recommendations for this user_id    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['environment statistics database - waste',\n",
       " 'use pmml to predict iris species',\n",
       " 'country statistics: infant mortality rate',\n",
       " 'airbnb data for analytics: vancouver calendar',\n",
       " 'fertility rate by country in total births per woman',\n",
       " 'using apply, sapply, lapply in r',\n",
       " 'airbnb data for analytics: chicago reviews',\n",
       " 'learn basics about notebooks and apache spark',\n",
       " 'discover, catalog and govern data with ibm data catalog',\n",
       " 'timeseries data analysis of iot events by using jupyter notebook',\n",
       " 'this week in data science (february 14, 2017)',\n",
       " 'use decision optimization to schedule league games',\n",
       " 'brunel: imitation is a sincere form of flattery',\n",
       " 'co2 emissions (metric tons per capita) by country',\n",
       " 'ibm watson machine learning: get started',\n",
       " 'airbnb data for analytics: austin calendar',\n",
       " 'manage object storage in dsx',\n",
       " 'finding the user in data science',\n",
       " 'airbnb data for analytics: athens calendar',\n",
       " 'trust in data science',\n",
       " 'web picks (december 2017)',\n",
       " 'uci: abalone',\n",
       " 'car performance data',\n",
       " 'create a project for watson machine learning in dsx',\n",
       " 'big data is better data',\n",
       " 'uci: car evaluation',\n",
       " 'pearson correlation aggregation on sparksql',\n",
       " 'making data cleaning simple with the sparkling.data library',\n",
       " 'using the maker palette in the ibm data science experience',\n",
       " 'web picks by dataminingapps',\n",
       " 'country population by gender 1985-2005',\n",
       " 'this week in data science (february 21, 2017)',\n",
       " 'using shell scripts to control data flows created in watson applications',\n",
       " 'education (2015): united states demographic measures',\n",
       " 'this week in data science (january 10, 2017)',\n",
       " 'advancements in the spark community',\n",
       " 'deep forest: towards an alternative to deep neural networks',\n",
       " 'introducing streams designer',\n",
       " 'nips 2016 — day 2 highlights',\n",
       " 'working with sqlite databases using python and pandas',\n",
       " 'annual % inflation by country',\n",
       " 'beyond parallelize and collect',\n",
       " 'this week in data science (march 7, 2017)',\n",
       " 'airbnb data for analytics: paris calendar',\n",
       " 'access mysql with r',\n",
       " \"a kaggler's guide to model stacking in practice\",\n",
       " 'annual % population growth by country',\n",
       " 'from scikit-learn model to cloud with wml client',\n",
       " 'what is hadoop?',\n",
       " 'notebooks: a power tool for data scientists',\n",
       " 'work with data connections in dsx',\n",
       " 'can a.i. be taught to explain itself?',\n",
       " 'the data processing inequality',\n",
       " 'from python nested lists to multidimensional numpy arrays',\n",
       " 'airbnb data for analytics: seattle reviews',\n",
       " 'analyze precipitation data',\n",
       " 'airbnb data for analytics: san francisco listings',\n",
       " 'country statistics: commercial bank prime lending rate',\n",
       " 'interest rates',\n",
       " 'a survey of books about apache spark™',\n",
       " 'ibm data science experience white paper - sparkr transforming r into a tool for big data analytics',\n",
       " 'how to ease the strain as your data volumes rise',\n",
       " 'analyze open data sets using pandas in a python notebook',\n",
       " 'web picks - dataminingapps',\n",
       " 'watson assistant workspace analysis with user logs',\n",
       " 'essentials of machine learning algorithms (with python and r codes)',\n",
       " 'apache spark 2.0: machine learning. under the hood and over the rainbow.',\n",
       " 'gradient boosting explained',\n",
       " 'dimensionality reduction algorithms',\n",
       " 'brunel in jupyter',\n",
       " 'what is text analytics?',\n",
       " 'airbnb data for analytics: venice listings',\n",
       " 'data visualization with ggplot2 cheat sheet',\n",
       " 'upload data and create data frames in jupyter notebooks',\n",
       " 'an attempt to understand boosting algorithm(s)',\n",
       " 'mapping points with folium',\n",
       " 'which one to choose for your problem',\n",
       " '10 essential algorithms for machine learning engineers',\n",
       " 'ensemble learning to improve machine learning results',\n",
       " 'refugees',\n",
       " 'airbnb data for analytics: montreal listings',\n",
       " 'agriculture, value added (% of gdp) by country',\n",
       " 'uci: poker hand - training data set',\n",
       " 'analyze ny restaurant data using spark in dsx',\n",
       " 'share the (pixiedust) magic – ibm watson data lab – medium',\n",
       " 'improving quality of life with spark-empowered machine learning',\n",
       " 'airbnb data for analytics: barcelona calendar',\n",
       " 'welcome to pixiedust',\n",
       " 'fortune 100 companies',\n",
       " '3992    using apache spark to predict attack vectors a...\\nName: title, dtype: object',\n",
       " 'load data into rstudio for analysis in dsx',\n",
       " 'laplace noising versus simulated out of sample methods (cross frames)',\n",
       " 'statistical bias types explained',\n",
       " 'breaking the 80/20 rule: how data catalogs transform data scientists’ productivity',\n",
       " 'working with notebooks in dsx',\n",
       " 'using machine learning to predict baseball injuries',\n",
       " 'use data assets in a project using ibm data catalog',\n",
       " 'how to get a job in deep learning',\n",
       " 'getting started with apache mahout',\n",
       " 'how to use version control (github) in rstudio within dsx?',\n",
       " 'analyze traffic data from the city of san francisco',\n",
       " 'introducing ibm watson studio ',\n",
       " '66855    migration from ibm bluemix data connect api (a...\\nName: title, dtype: object',\n",
       " 'python machine learning: scikit-learn tutorial',\n",
       " 'airbnb data for analytics: washington d.c. reviews',\n",
       " 'intents & examples for ibm watson conversation',\n",
       " 'run dsx notebooks on amazon emr',\n",
       " 'recommender systems: approaches & algorithms',\n",
       " 'airbnb data for analytics: trentino calendar',\n",
       " 'analyze starcraft ii replays with jupyter notebooks',\n",
       " 'data visualization playbook: telling the data story',\n",
       " 'python if statements explained (python for data science basics #4)',\n",
       " 'airbnb data for analytics: berlin reviews',\n",
       " 'airbnb data for analytics: sydney listings',\n",
       " 'small steps to tensorflow',\n",
       " 'customers of a telco including services used',\n",
       " 'country statistics: electricity - consumption',\n",
       " 'analyze open data sets with pandas dataframes',\n",
       " 'country statistics: industrial production growth rate',\n",
       " 'country statistics: airports',\n",
       " 'country statistics: stock of broad money',\n",
       " 'forest area by country in sq km',\n",
       " '10 data science, machine learning and ai podcasts you must listen to',\n",
       " 'airbnb data for analytics: oakland reviews',\n",
       " 'airbnb data for analytics: antwerp calendar',\n",
       " \"feature importance and why it's important\",\n",
       " 'automating web analytics through python',\n",
       " 'airbnb data for analytics: vienna reviews',\n",
       " 'an introduction to stock market data analysis with r (part 1)',\n",
       " 'this week in data science (may 30, 2017)',\n",
       " 'foundational methodology for data science',\n",
       " 'from local spark mllib model to cloud with watson machine learning',\n",
       " 'data visualization with r: scrum metrics',\n",
       " 'rapidly build machine learning flows with dsx',\n",
       " 'united states demographic measures: education',\n",
       " 'access mysql with python',\n",
       " 'run shiny applications in rstudio in dsx',\n",
       " 'pixiedust gets its first community-driven feature in 1.0.4',\n",
       " 'analyze accident reports on amazon emr spark',\n",
       " 'leaflet: interactive web maps with r',\n",
       " 'airbnb data for analytics: vancouver listings',\n",
       " 'household consumption expenditure',\n",
       " 'ml optimization using cognitive assistant',\n",
       " 'statistical bias types explained (with examples)',\n",
       " 'machine learning exercises in python, part 1',\n",
       " \"december '16 rstudio tips and tricks\",\n",
       " 'wages',\n",
       " 'country statistics: railways',\n",
       " '8 ways to turn data into value with apache spark machine learning',\n",
       " 'tidy data in python',\n",
       " 'country statistics: crude oil - imports',\n",
       " 'airbnb data for analytics: dublin reviews',\n",
       " 'marital status of men and women',\n",
       " 'mobile-cellular telephone subscriptions per 100 inhabitants, worldwide',\n",
       " 'airbnb data for analytics: toronto reviews',\n",
       " 'country statistics: gross national saving',\n",
       " 'airbnb data for analytics: washington d.c. calendar',\n",
       " 'three reasons machine learning models go out of sync',\n",
       " 'a classification problem',\n",
       " 'how to perform a logistic regression in r',\n",
       " 'the nurse assignment problem',\n",
       " 'governance overview for ibm data catalog',\n",
       " 'recommendation system algorithms – stats and bots',\n",
       " 'tidyr 0.6.0',\n",
       " 'working with db2 warehouse on cloud in data science experience',\n",
       " 'get started with streams designer by following this roadmap',\n",
       " 'enjoy python 3.5 in jupyter notebooks',\n",
       " 'i am not a data scientist – ibm watson data lab',\n",
       " 'country statistics: internet users',\n",
       " 'pixiedust 1.0 is here! – ibm watson data lab',\n",
       " 'do i need to learn r?',\n",
       " '520    using notebooks with pixiedust for fast, flexi...\\nName: title, dtype: object',\n",
       " 'modeling energy usage in new york city',\n",
       " 'airbnb data for analytics: brussels reviews',\n",
       " 'a tensorflow regression model to predict house values',\n",
       " 'country statistics: telephones - mobile cellular',\n",
       " 'using bigdl in dsx for deep learning on spark',\n",
       " 'the random forest algorithm ',\n",
       " '7292    a dramatic tour through python’s data visualiz...\\nName: title, dtype: object',\n",
       " 'maximize oil company profits',\n",
       " 'neural language modeling from scratch (part 1)',\n",
       " 'top 10 machine learning use cases: part 1',\n",
       " 'energy use (kg of oil equivalent per capita) by country',\n",
       " 'consumption of ozone-depleting cfcs in odp metric tons',\n",
       " 'brunel interactive visualizations in jupyter notebooks',\n",
       " 'random forest interpretation – conditional feature contributions',\n",
       " 'military expenditure as % of gdp by country',\n",
       " 'poverty (2015): united states demographic measures',\n",
       " 'a glimpse inside the mind of a data scientist',\n",
       " '15 page tutorial for r',\n",
       " 'experience iot with coursera',\n",
       " 'government consumption expenditure',\n",
       " 'airbnb data for analytics: vancouver reviews',\n",
       " 'international liquidity',\n",
       " 'the greatest public datasets for ai – startup grind',\n",
       " 'tidyverse practice: mapping large european cities',\n",
       " 'time series anomaly detection algorithms – stats and bots',\n",
       " 'watson machine learning for developers',\n",
       " '502    forgetting the past to learn the future: long ...\\nName: title, dtype: object',\n",
       " 'challenges in deep learning',\n",
       " 'country statistics: children under the age of 5 years underweight',\n",
       " 'country statistics: life expectancy at birth',\n",
       " 'social media insights with watson developer cloud & watson studio',\n",
       " 'country statistics: stock of domestic credit',\n",
       " 'use spark for python to load data and run sql queries',\n",
       " 'country statistics: distribution of family income - gini index',\n",
       " 'contraceptive prevalence (% women 15-49) by country',\n",
       " 'create a project in dsx',\n",
       " 'airbnb data for analytics: athens reviews',\n",
       " 'airbnb data for analytics: nashville calendar',\n",
       " 'access postgresql with python',\n",
       " 'join and enrich data from multiple sources',\n",
       " 'creating the data science experience',\n",
       " 'ggplot2 2.2.0 coming soon!',\n",
       " 'airbnb data for analytics: venice reviews',\n",
       " '10 data science podcasts you need to be listening to right now',\n",
       " 'publish notebooks to github in dsx',\n",
       " 'use apache systemml and spark for machine learning',\n",
       " 'data science in the cloud',\n",
       " 'machine learning and the science of choosing',\n",
       " 'housing (2015): united states demographic measures',\n",
       " 'airbnb data for analytics: vienna calendar',\n",
       " 'model bike sharing data with spss',\n",
       " 'airbnb data for analytics: san diego listings',\n",
       " 'word2vec in data products',\n",
       " 'occupation (2015): united states demographic measures',\n",
       " '10 tips on using jupyter notebook',\n",
       " 'top 20 r machine learning and data science packages',\n",
       " 'airbnb data for analytics: paris reviews',\n",
       " 'package development with devtools  cheat sheet',\n",
       " 'use xgboost, scikit-learn & ibm watson machine learning apis',\n",
       " 'how to choose a project to practice data science',\n",
       " 'predicting churn with the spss random tree algorithm',\n",
       " 'part-time employment rate, worldwide, by country and year',\n",
       " 'dt: an r interface to the datatables library',\n",
       " 'airbnb data for analytics: portland reviews',\n",
       " 'fighting gerrymandering: using data science to draw fairer congressional districts',\n",
       " 'simple linear regression? do it the bayesian way',\n",
       " 'this week in data science (april 4, 2017)',\n",
       " 'webinar: april 11 - thinking inside the box: you can do that inside a data frame?!',\n",
       " 'xml2 1.0.0',\n",
       " 'r markdown reference guide',\n",
       " 'what caused the challenger disaster?',\n",
       " '1448    i ranked every intro to data science course on...\\nName: title, dtype: object',\n",
       " 'graph-based machine learning',\n",
       " 'this week in data science (may 2, 2017)',\n",
       " 'twelve\\xa0ways to color a map of africa using brunel',\n",
       " 'airbnb data for analytics: sydney reviews',\n",
       " 'perform sentiment analysis with lstms, using tensorflow',\n",
       " 'life expectancy at birth by country in total years',\n",
       " 'connect to db2 warehouse on cloud and db2 using scala',\n",
       " 'building custom machine learning algorithms with apache systemml',\n",
       " 'this week in data science (march 28, 2017)',\n",
       " 'neurally embedded emojis',\n",
       " 'common excel tasks demonstrated in\\xa0pandas',\n",
       " 'an introduction to scientific python (and a bit of the maths behind it) – numpy',\n",
       " 'this week in data science (january 24, 2017)',\n",
       " 'a comparison of logistic regression and naive bayes ',\n",
       " 'breast cancer detection with xgboost, wml and scikit',\n",
       " 'overfitting in machine learning: what it is and how to prevent it',\n",
       " 'airbnb data for analytics: austin reviews',\n",
       " 'country statistics: electricity - from fossil fuels',\n",
       " 'a moving average trading strategy',\n",
       " 'apache spark as the new engine of genomics',\n",
       " 'country statistics: unemployment rate',\n",
       " 'uci: adult - predict income',\n",
       " 'births attended by skilled health staff (% of total) by country',\n",
       " '5 practical use cases of social network analytics: going beyond facebook and twitter',\n",
       " 'data model with streaming analytics and python',\n",
       " '8170    data science expert interview: dez blanchfield...\\nName: title, dtype: object',\n",
       " '0 to life-changing app: new apache systemml api on spark shell',\n",
       " 'house building with worker skills',\n",
       " 'the nurse assignment problem data',\n",
       " 'shiny 0.13.0',\n",
       " 'use sql with data in hadoop python',\n",
       " 'airbnb data for analytics: portland calendar',\n",
       " 'airbnb data for analytics: seattle calendar',\n",
       " 'when machine learning matters · erik bernhardsson',\n",
       " 'airbnb data for analytics: boston reviews',\n",
       " 'empirical bayes for multiple sample sizes',\n",
       " 'this week in data science (april 18, 2017)',\n",
       " '10 powerful features on watson data platform, no coding necessary',\n",
       " 'watson speech-to-text services — tl;dr need not apply',\n",
       " 'working with ibm cloud object storage in python',\n",
       " 'got zip code data? prep it for analytics. – ibm watson data lab – medium',\n",
       " 'introduction to neural networks, advantages and applications',\n",
       " 'optimization for deep learning highlights in 2017',\n",
       " 'the difference between ai, machine learning, and deep learning?',\n",
       " 'use deep learning for image classification',\n",
       " 'data structures related to machine learning algorithms',\n",
       " 'airbnb data for analytics: antwerp listings',\n",
       " 'jupyter notebooks with scala, python, or r kernels',\n",
       " 'what is spark?',\n",
       " \"a beginner's guide to variational methods\",\n",
       " 'reducing overplotting in scatterplots',\n",
       " 'fashion-mnist',\n",
       " 'annual precipitation by country 1990-2009',\n",
       " 'predicting gentrification using longitudinal census data',\n",
       " 'talent vs luck: the role of randomness in success and failure',\n",
       " 'this week in data science (october 18, 2016)',\n",
       " 'apple, ibm add machine learning to partnership with watson-core ml coupling',\n",
       " 'pixieapp for outlier detection',\n",
       " 'time series analysis using max/min and neuroscience',\n",
       " 'overlapping co-cluster recommendation algorithm (ocular)',\n",
       " 'geographic coordinates of world locations',\n",
       " 'data science of variable selection',\n",
       " 'a guide to convolution arithmetic for deep learning',\n",
       " 'the 3 kinds of context: machine learning and the art of the frame',\n",
       " 'roads paved as % of total roads by country',\n",
       " 'greenhouse gas emissions worldwide',\n",
       " 'sudoku',\n",
       " 'getting started with python',\n",
       " 'enhanced color mapping',\n",
       " '20405    how to tame the valley — hessian-free hacks fo...\\nName: title, dtype: object',\n",
       " 'data visualization playbook: the right level of detail',\n",
       " 'apache spark @scale: a 60 tb+ production use case',\n",
       " 'data science bowl 2017',\n",
       " 'improving the roi of big data and analytics through leveraging new sources of data',\n",
       " 'probabilistic graphical models tutorial\\u200a—\\u200apart 1 – stats and bots',\n",
       " 'airbnb data for analytics: boston calendar',\n",
       " 'what is smote in an imbalanced class setting (e.g. fraud detection)?',\n",
       " 'collect your own fitbit data with python',\n",
       " 'readr 1.0.0',\n",
       " 'pixiedust: magic for your python notebook',\n",
       " 'awesome deep learning papers',\n",
       " 'visual information theory ',\n",
       " 'ibm data catalog is now generally available',\n",
       " 'airbnb data for analytics: new orleans reviews',\n",
       " 'putting a human face on machine learning',\n",
       " 'how to use db2 warehouse on cloud in data science experience notebooks',\n",
       " 'use spark r to load and analyze data',\n",
       " 'uci ml repository: chronic kidney disease data set',\n",
       " 'data visualization playbook: revisiting the basics',\n",
       " 'leverage python, scikit, and text classification for behavioral profiling',\n",
       " 'data science experience documentation',\n",
       " 'uci: white wine quality',\n",
       " 'country statistics - europe - population and society',\n",
       " 'finding optimal locations of new store using decision optimization',\n",
       " 'process events from the watson iot platform in a streams python application',\n",
       " 'deep learning, structure and innate priors',\n",
       " 'country statistics: health expenditures',\n",
       " 'jupyter notebook tutorial',\n",
       " 'ml algorithm != learning machine',\n",
       " 'worldwide county and region - national accounts - gross national income 1948-2010',\n",
       " 'airbnb data for analytics: amsterdam reviews',\n",
       " 'build a python app on the streaming analytics service',\n",
       " 'uci: heart disease - cleveland',\n",
       " 'dry bulb temperature, by country, station and year',\n",
       " 'make machine learning a reality for your enterprise',\n",
       " 'analyze data, build a dashboard with spark and pixiedust',\n",
       " 'you could be looking at it all wrong',\n",
       " 'model a golomb ruler',\n",
       " 'd3heatmap: interactive heat maps',\n",
       " 'classify tumors with machine learning',\n",
       " 'measles immunization % children 12-23 months by country',\n",
       " 'deep learning with tensorflow course by big data university',\n",
       " 'using pixiedust for fast, flexible, and easier data analysis and experimentation',\n",
       " 'predicting the 2016 us presidential election',\n",
       " 'income (2015): united states demographic measures',\n",
       " 'external debt stocks, total (dod, current us$) by country',\n",
       " 'this week in data science (july 26, 2016)',\n",
       " 'country surface area (sq. km)',\n",
       " 'breast cancer wisconsin (diagnostic) data set',\n",
       " 'drowning in data sources: how data cataloging could fix your findability problems',\n",
       " 'airbnb data for analytics: portland listings',\n",
       " 'machine learning for everyone',\n",
       " 'how to scale your analytics using r',\n",
       " 'country statistics: natural gas - consumption',\n",
       " 'using rstudio in ibm data science experience',\n",
       " 'probabilistic graphical models tutorial\\u200a—\\u200apart 2 – stats and bots',\n",
       " 'airbnb data for analytics: antwerp reviews',\n",
       " '10 must attend data science, ml and ai conferences in 2018',\n",
       " 'uci: iris',\n",
       " 'what is systemml? why is it relevant to you?',\n",
       " 'calls by customers of a telco company',\n",
       " 'r for data science',\n",
       " 'deep learning achievements over the past year ',\n",
       " 'build deep learning architectures with neural network modeler',\n",
       " 'simple graphing with ipython and\\xa0pandas',\n",
       " 'are your predictive models like broken clocks?',\n",
       " 'what’s new in the streaming analytics service on bluemix',\n",
       " 'airbnb data for analytics: vienna listings',\n",
       " 'labor',\n",
       " 'why you should master r (even if it might eventually become obsolete)',\n",
       " 'using deep learning to reconstruct high-resolution audio',\n",
       " 'analyzing data by using the sparkling.data library features',\n",
       " 'uci: wine recognition',\n",
       " 'apache spark sql analyzer resolves order-by column',\n",
       " 'get social with your notebooks in dsx',\n",
       " 'cleaning the swamp: turn your data lake into a source of crystal-clear insight',\n",
       " 'calculate moving averages on real time data with streams designer',\n",
       " 'airbnb data for analytics: trentino listings',\n",
       " 'super fast string matching in python',\n",
       " 'this week in data science (february 7, 2017)',\n",
       " 'airbnb data for analytics: london listings',\n",
       " 'visualising data the node.js way',\n",
       " '7 types of job profiles that makes you a data scientist',\n",
       " 'this week in data science (january 31, 2017)',\n",
       " 'learning statistics on youtube',\n",
       " 'country statistics: population',\n",
       " 'this week in data science (august 02, 2016)',\n",
       " 'united states demographic measures: zip code tabulation areas (zctas)',\n",
       " 'airbnb data for analytics: toronto listings',\n",
       " 'analyze energy consumption in buildings',\n",
       " 'web picks (week of 23 january 2017)',\n",
       " 'world tourism data by the world tourism organization',\n",
       " 'use iot data in streams designer for billing and alerts',\n",
       " 'customer demographics and sales',\n",
       " 'seti data, publicly available, from ibm',\n",
       " 'the new builders podcast ep 3: collaboration',\n",
       " 'total employment, by economic activity (thousands)',\n",
       " 'airbnb data for analytics: mallorca reviews',\n",
       " 'this week in data science (february 28, 2017)',\n",
       " 'airbnb data for analytics: santa cruz county reviews',\n",
       " 'households by type of household, age and sex of head of household',\n",
       " '70 amazing free data sources you should know',\n",
       " 'a fast on-disk format for data frames for r and python, powered by apache arrow',\n",
       " \"for ai to get creative, it must learn the rules--then how to break 'em\",\n",
       " 'web picks (week of 2 october 2017)',\n",
       " 'this week in data science',\n",
       " 'a dynamic duo – inside machine learning – medium',\n",
       " 'intelligent applications - apache spark',\n",
       " 'open sourcing 223gb of driving data – udacity inc',\n",
       " 'healthcare python streaming application demo',\n",
       " 'generative adversarial networks (gans)',\n",
       " 'making data science a team sport',\n",
       " 'using machine learning to predict parking difficulty',\n",
       " 'this week in data science (may 16, 2017)',\n",
       " 'use ibm data science experience to detect time series anomalies',\n",
       " 'country statistics: central bank discount rate',\n",
       " 'data science experience demo: modeling energy usage in nyc',\n",
       " 'spark 2.1 and job monitoring available in dsx',\n",
       " 'develop a scala spark model on chicago building violations',\n",
       " 'use the cloudant-spark connector in python notebook',\n",
       " 'a day in the life of a data engineer',\n",
       " 'bayesian regularization for #neuralnetworks – autonomous agents\\u200a—\\u200a#ai',\n",
       " '10 pieces of advice to beginner data scientists',\n",
       " '3 scenarios for machine learning on multicloud',\n",
       " 'why even a moth’s brain is smarter than an ai',\n",
       " 'gosales transactions for naive bayes model',\n",
       " 'shaping data with ibm data refinery',\n",
       " \"2875    hugo larochelle's neural network & deep learni...\\nName: title, dtype: object\",\n",
       " 'tensorflow quick tips',\n",
       " 'using deep learning with keras to predict customer churn',\n",
       " 'predict loan applicant behavior with tensorflow neural networking',\n",
       " 'higher-order logistic regression for large datasets',\n",
       " 'artificial intelligence, ethically speaking – inside machine learning – medium',\n",
       " 'country statistics: roadways',\n",
       " 'neural networks for beginners: popular types and applications',\n",
       " 'world marriage data',\n",
       " 'improving real-time object detection with yolo',\n",
       " 'shiny: a data scientist’s best friend',\n",
       " 'clustering: a guide for the perplexed',\n",
       " 'jupyter (ipython) notebooks features',\n",
       " 'intentional homicide, number and rate per 100,000 population, by country',\n",
       " 'the million dollar question: where is my data?',\n",
       " 'airbnb data for analytics: washington d.c. listings',\n",
       " 'back to basics — jupyter notebooks',\n",
       " 'city population by sex, city and city type',\n",
       " 'web picks (week of 4 september 2017)',\n",
       " \"let's have some fun with nfl data\",\n",
       " 'airbnb data for analytics: austin listings',\n",
       " 'statistics for hackers',\n",
       " 'gosales transactions for logistic regression model',\n",
       " 'use the machine learning library',\n",
       " 'building a business that combines human experts and data science',\n",
       " 'self-service data preparation with ibm data refinery',\n",
       " 'unmet need for family planning, spacing, percentage, worldwide, by country',\n",
       " 'data visualization: the importance of excluding unnecessary details',\n",
       " 'one year as a data scientist at stack overflow',\n",
       " 'country statistics: budget surplus or deficit',\n",
       " 'hyperparameter optimization: sven hafeneger',\n",
       " 'using github for project control in dsx',\n",
       " 'airbnb data for analytics: trentino reviews',\n",
       " 'airbnb data for analytics: chicago calendar',\n",
       " 'foreign direct investment, net inflows (bop, current us$) by country',\n",
       " 'making sense of the bias / variance trade-off in (deep) reinforcement learning',\n",
       " '68879    don’t throw more data at the problem! here’s h...\\nName: title, dtype: object',\n",
       " 'primary school completion rate % of relevant age group by country',\n",
       " 'using dsx notebooks to analyze github data',\n",
       " 'analyze open data sets with spark & pixiedust',\n",
       " 'flexdashboard: interactive dashboards for r',\n",
       " 'working interactively with rstudio and notebooks in dsx',\n",
       " 'score a predictive model built with ibm spss modeler, wml & dsx',\n",
       " 'best packages for data manipulation in r',\n",
       " 'airbnb data for analytics: new york city reviews',\n",
       " 'hurricane how-to',\n",
       " '0 to life-changing app: scala first steps and an interview with jakob odersky',\n",
       " 'ibm watson facebook posts for 2015',\n",
       " 'use spark for r to load data and run sql queries',\n",
       " 'discover hidden facebook usage insights',\n",
       " 'the data science process',\n",
       " 'learn about data science in world of watson',\n",
       " 'apache spark lab, part 3: machine learning',\n",
       " 'data science platforms are on the rise and ibm is leading the way',\n",
       " 'modern machine learning algorithms',\n",
       " 'using machine learning to predict value of homes on airbnb',\n",
       " 'best practices for custom models in watson visual recognition',\n",
       " 'analyze db2 warehouse on cloud data in rstudio in dsx',\n",
       " 'data wrangling with dplyr and tidyr cheat sheet',\n",
       " 'effectively using\\xa0matplotlib',\n",
       " 'employed population by occupation and age',\n",
       " 'electric power consumption (kwh per capita) by country',\n",
       " 'time series prediction using recurrent neural networks (lstms)',\n",
       " 'the pandas data analysis library',\n",
       " 'brunel 2.0 preview',\n",
       " 'airbnb data for analytics: san diego reviews',\n",
       " 'visualize the 1854 london cholera outbreak',\n",
       " 'country statistics: market value of publicly traded shares',\n",
       " 'tidy up your jupyter notebooks with scripts',\n",
       " 'recent trends in recommender systems',\n",
       " 'load and analyze public data sets in dsx',\n",
       " 'mycheatsheets.com',\n",
       " 'airbnb data for analytics: boston listings',\n",
       " 'gross national income per capita, atlas method (current us$) by country',\n",
       " 'ratio (% of population) at national poverty line by country',\n",
       " 'uci: red wine quality',\n",
       " 'flightpredict ii: the sequel  – ibm watson data lab',\n",
       " 'the t-distribution: a key statistical concept discovered by a beer brewery',\n",
       " 'the unit commitment problem',\n",
       " 'predict chronic kidney disease using spss modeler flows',\n",
       " 'adoption of machine learning to software failure prediction',\n",
       " 'bayesian nonparametric models – stats and bots',\n",
       " 'country statistics: imports',\n",
       " 'programmatic evaluation using watson conversation',\n",
       " 'airbnb data for analytics: athens listings',\n",
       " 'access db2 warehouse on cloud and db2 with python',\n",
       " 'read and write data to and from amazon s3 buckets in rstudio',\n",
       " 'python for loops explained (python for data science basics #5)',\n",
       " 'imitation learning in tensorflow (hopper from openai gym)',\n",
       " 'a new version of dt (0.2) on cran',\n",
       " 'predicting flight cancellations using weather data, part 3',\n",
       " 'build a logistic regression model with wml & dsx',\n",
       " 'practical tutorial on random forest and parameter tuning in r',\n",
       " 'environment statistics database - water',\n",
       " 'the new builders ep. 13: all the data that’s fit to analyze',\n",
       " 'this week in data science (december 27, 2016)',\n",
       " 'spark 1.4 for rstudio',\n",
       " 'mobile cellular subscriptions per 100 people by country',\n",
       " 'country populations 15 years of age and over, by educational attainment, age and sex',\n",
       " 'worldwide fuel oil consumption by household (in 1000 metric tons)',\n",
       " 'country statistics: telephones - fixed lines',\n",
       " 'movie recommender system with spark machine learning',\n",
       " 'build a predictive analytic model',\n",
       " 'htmlwidgets: javascript data visualization for r',\n",
       " 'apache spark™ 2.0: migrating applications',\n",
       " 'an interview with pythonista katharine jarmul',\n",
       " 'percentage of internet users by country',\n",
       " 'airbnb data for analytics: sydney calendar',\n",
       " 'a visual explanation of the back propagation algorithm for neural networks',\n",
       " 'configuring the apache spark sql context',\n",
       " 'build a naive-bayes model with wml & dsx',\n",
       " 'let data dictate the visualization',\n",
       " 'quick guide to build a recommendation engine in python',\n",
       " 'top 10 machine learning algorithms for beginners',\n",
       " 'apache spark lab, part 1: basic concepts',\n",
       " 'optimizing a marketing campaign: moving from predictions to actions',\n",
       " 'categorize urban density',\n",
       " 'ibm cloud sql query',\n",
       " 'variational auto-encoder for \"frey faces\" using keras',\n",
       " 'upload files to ibm data science experience using the command line',\n",
       " 'interconnect with us',\n",
       " 'how can data scientists collaborate to build better business',\n",
       " 'ibm data catalog overview',\n",
       " '51822    using apache spark as a parallel processing fr...\\nName: title, dtype: object',\n",
       " 'new shiny cheat sheet and video tutorial',\n",
       " 'airbnb data for analytics: berlin calendar',\n",
       " 'airbnb data for analytics: amsterdam calendar',\n",
       " 'airbnb data for analytics: barcelona reviews',\n",
       " 'find airbnb deals in portland with machine learning using r',\n",
       " 'uci: forest fires',\n",
       " 'collecting data science cheat sheets',\n",
       " 'using brunel in ipython/jupyter notebooks',\n",
       " 'airbnb data for analytics: antwerp listings test',\n",
       " 'working with on-premises databases — step by step',\n",
       " 'missing data conundrum: exploration and imputation techniques',\n",
       " 'access postgresql with r',\n",
       " 'learn tensorflow and deep learning together and now!',\n",
       " 'why relational databases and r?',\n",
       " 'spark-based machine learning tools for capturing word meanings',\n",
       " 'a guide to receptive field arithmetic for convolutional neural networks',\n",
       " 'this week in data science (april 11, 2017)',\n",
       " 'country statistics: gdp - per capita (ppp)',\n",
       " '1357    what i learned implementing a classifier from ...\\nName: title, dtype: object',\n",
       " 'deploy your python model as a restful api',\n",
       " 'data tidying in data science experience',\n",
       " 'refugees, worldwide, 2003 - 2013',\n",
       " 'developing ibm streams applications with the python api (version 1.6)',\n",
       " 'health insurance (2015): united states demographic measures',\n",
       " 'from spark ml model to online scoring with scala',\n",
       " 'cifar-100 - python version',\n",
       " 'what’s new in data refinery?',\n",
       " 'how ibm builds an effective data science team',\n",
       " 'working with ibm cloud object storage in r',\n",
       " 'style transfer experiments with watson machine learning',\n",
       " 'country statistics: crude oil - exports',\n",
       " 'how to map usa rivers using ggplot2',\n",
       " 'real-time sentiment analysis of twitter hashtags with spark (+ pixiedust)',\n",
       " 'pulling and displaying etf data',\n",
       " 'country statistics: maternal mortality rate',\n",
       " '54174    detect potentially malfunctioning sensors in r...\\nName: title, dtype: object',\n",
       " 'this week in data science (april 25, 2017)',\n",
       " 'visualize car data with brunel',\n",
       " 'shiny 0.12: interactive plots with ggplot2',\n",
       " 'generalization in deep learning',\n",
       " 'create a connection and add it to a project using ibm data refinery',\n",
       " 'population below national poverty line, total, percentage',\n",
       " 'the machine learning database',\n",
       " '9 mistakes to avoid when starting your career in data science',\n",
       " 'how to solve 90% of nlp problems',\n",
       " 'machine learning for the enterprise.',\n",
       " 'uci: poker hand - testing data set',\n",
       " 'continuous learning on watson',\n",
       " 'total population by country',\n",
       " 'building your first machine learning system ',\n",
       " 'interactive web apps with shiny cheat sheet',\n",
       " 'developing for the ibm streaming analytics service',\n",
       " 'deep learning with data science experience',\n",
       " 'better together: spss and data science experience',\n",
       " 'top analytics tools in 2016',\n",
       " 'unstructured and structured data versus repetitive and non-repetitive',\n",
       " 'country statistics: merchant marine',\n",
       " 'don’t overlook simpler techniques and algorithms',\n",
       " 'interactive time series with dygraphs',\n",
       " 'apache spark: upgrade and speed-up your analytics',\n",
       " \"h2o with ibm's data science experience (dsx)\",\n",
       " 'airbnb data for analytics: chicago listings',\n",
       " 'dplyr 0.5.0',\n",
       " 'excel files: loading from object storage — python',\n",
       " 'understanding empirical bayes estimation (using baseball statistics)',\n",
       " 'access ibm analytics for apache spark from rstudio',\n",
       " \"i'd rather predict basketball games than elections: elastic nba rankings\",\n",
       " 'insights from new york car accident reports',\n",
       " 'backpropagation — how neural networks learn complex behaviors',\n",
       " 'use spark for scala to load data and run sql queries',\n",
       " 'airbnb data for analytics: madrid listings',\n",
       " 'use r dataframes & ibm watson natural language understanding',\n",
       " 'deep learning trends and an example',\n",
       " 'accelerate your workflow with dsx',\n",
       " 'blogging with brunel',\n",
       " 'how open api economy accelerates the growth of big data and analytics',\n",
       " 'united states demographic measures: income',\n",
       " 'use ibm data science experience to read and write data stored on amazon s3',\n",
       " 'markdown for jupyter notebooks cheatsheet',\n",
       " 'sector correlations shiny app',\n",
       " 'load db2 warehouse on cloud data with apache spark in dsx',\n",
       " 'natural gas production, 1995 - 2012, worldwide',\n",
       " 'what is machine learning?',\n",
       " 'declarative machine learning',\n",
       " 'rstudio ide  cheat sheet',\n",
       " 'country statistics: reserves of foreign exchange and gold',\n",
       " 'cifar-10 - python version',\n",
       " 'times world university ranking analysis',\n",
       " 'cache table in apache spark sql',\n",
       " 'machine learning for the enterprise',\n",
       " 'country statistics: crude oil - proved reserves',\n",
       " 'leverage scikit-learn models with core ml',\n",
       " 'migrating to python 3 with pleasure',\n",
       " 'airbnb data for analytics: new york city calendar',\n",
       " 'data science expert interview: holden karau',\n",
       " 'how smart catalogs can turn the big data flood into an ocean of opportunity',\n",
       " 'airbnb data for analytics: london reviews',\n",
       " 'this week in data science (may 23, 2017)',\n",
       " 'the art of side effects: curing apache spark streaming’s amnesia (part 1/2)',\n",
       " 'airbnb data for analytics: new orleans listings',\n",
       " 'getting started with graphframes in apache spark™',\n",
       " 'transfer learning for flight delay prediction via variational autoencoders',\n",
       " 'visualize data with the matplotlib library',\n",
       " 'aspiring data scientists! start to learn statistics with these 6 books!',\n",
       " 'apache spark™ 2.0: extend structured streaming for spark ml',\n",
       " 'country statistics: refined petroleum products - production',\n",
       " 'pseudo-labeling a simple semi-supervised learning method',\n",
       " 'transform anything into a vector',\n",
       " 'this week in data science (november 22, 2016)',\n",
       " 'this week in data science (january 17, 2017)',\n",
       " 'easy json loading and social sharing in dsx notebooks',\n",
       " 'apache spark lab, part 2: querying data',\n",
       " 'sparklyr — r interface for apache spark',\n",
       " 'announcing dsx environments in beta!',\n",
       " 'detect malfunctioning iot sensors with streaming analytics',\n",
       " 'how the circle line rogue train was caught with data',\n",
       " 'consumer prices',\n",
       " 'analyze facebook data using ibm watson and watson studio',\n",
       " 'worldwide electricity demand and production 1990-2012',\n",
       " 'the two phases of gradient descent in deep learning',\n",
       " 'data science for real-time streaming analytics',\n",
       " 'spark sql - rapid performance evolution',\n",
       " 'airbnb data for analytics: toronto calendar',\n",
       " 'airbnb data for analytics: barcelona listings',\n",
       " 'united states demographic measures: population and age',\n",
       " 'apache spark™ 2.0: impressive improvements to spark sql',\n",
       " 'uci: sms spam collection',\n",
       " 'introduction to market basket analysis in\\xa0python',\n",
       " 'apache systemml',\n",
       " 'the power of machine learning in spark',\n",
       " 'workflow in r',\n",
       " 'persistent changes to spark config in dsx',\n",
       " 'airbnb data for analytics: venice calendar',\n",
       " 'airbnb data for analytics: paris listings',\n",
       " 'how to write the first for loop in r',\n",
       " 'country statistics: current account balance',\n",
       " 'some random weekend reading',\n",
       " 'ingest data from message hub in a streams flow',\n",
       " 'adolescent fertility rate (births per 1,000 women ages 15-19), worldwide',\n",
       " 'this week in data science (november 01, 2016)',\n",
       " 'dsx: hybrid mode',\n",
       " 'airbnb data for analytics: amsterdam listings',\n",
       " 'deep learning from scratch i: computational graphs',\n",
       " 'stacking multiple custom models in watson visual recognition',\n",
       " 'analyzing streaming data from kafka topics',\n",
       " 'high-tech exports as % of manufactured exports by country',\n",
       " 'improved water source by country: % population with access',\n",
       " '56594    lifelong (machine) learning: how automation ca...\\nName: title, dtype: object',\n",
       " 'roads, paved (% of total roads), worldwide, 1990-2011']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Results\n",
    "get_article_names(user_user_recs(1, 10)) # Return 10 recommendations for user 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If this is all you see, you passed all of our tests!  Nice job!\n"
     ]
    }
   ],
   "source": [
    "# Test your functions here - No need to change this code - just run this cell\n",
    "assert set(get_article_names(['1024.0', '1176.0', '1305.0', '1314.0', '1422.0', '1427.0'])) == set(['using deep learning to reconstruct high-resolution audio', 'build a python app on the streaming analytics service', 'gosales transactions for naive bayes model', 'healthcare python streaming application demo', 'use r dataframes & ibm watson natural language understanding', 'use xgboost, scikit-learn & ibm watson machine learning apis']), \"Oops! Your the get_article_names function doesn't work quite how we expect.\"\n",
    "assert set(get_article_names(['1320.0', '232.0', '844.0'])) == set(['housing (2015): united states demographic measures','self-service data preparation with ibm data refinery','use the cloudant-spark connector in python notebook']), \"Oops! Your the get_article_names function doesn't work quite how we expect.\"\n",
    "assert set(get_user_articles(20)[0]) == set(['1320.0', '232.0', '844.0'])\n",
    "assert set(get_user_articles(20)[1]) == set(['housing (2015): united states demographic measures', 'self-service data preparation with ibm data refinery','use the cloudant-spark connector in python notebook'])\n",
    "assert set(get_user_articles(2)[0]) == set(['1024.0', '1176.0', '1305.0', '1314.0', '1422.0', '1427.0'])\n",
    "assert set(get_user_articles(2)[1]) == set(['using deep learning to reconstruct high-resolution audio', 'build a python app on the streaming analytics service', 'gosales transactions for naive bayes model', 'healthcare python streaming application demo', 'use r dataframes & ibm watson natural language understanding', 'use xgboost, scikit-learn & ibm watson machine learning apis'])\n",
    "print(\"If this is all you see, you passed all of our tests!  Nice job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` Now we are going to improve the consistency of the **user_user_recs** function from above.  \n",
    "\n",
    "* Instead of arbitrarily choosing when we obtain users who are all the same closeness to a given user - choose the users that have the most total article interactions before choosing those with fewer article interactions.\n",
    "\n",
    "\n",
    "* Instead of arbitrarily choosing articles from the user where the number of recommended articles starts below m and ends exceeding m, choose articles with the articles with the most total interactions before choosing those with fewer total interactions. This ranking should be  what would be obtained from the **top_articles** function you wrote earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_sorted_users(user_id, df=df, user_item=user_item):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int)\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook \n",
    "    user_item - (pandas dataframe) matrix of users by articles: \n",
    "            1's when a user has interacted with an article, 0 otherwise\n",
    "    \n",
    "            \n",
    "    OUTPUT:\n",
    "    neighbors_df - (pandas dataframe) a dataframe with:\n",
    "                    neighbor_id - is a neighbor user_id\n",
    "                    similarity - measure of the similarity of each user to the provided user_id\n",
    "                    num_interactions - the number of articles viewed by the user - if a u\n",
    "                    \n",
    "    Other Details - sort the neighbors_df by the similarity and then by number of interactions where \n",
    "                    highest of each is higher in the dataframe\n",
    "     \n",
    "    '''\n",
    "    # Your code here\n",
    "    \n",
    "    user_interactions = df.groupby(['user_id'])['article_id'].count()\n",
    "    \n",
    "    neighbor_id = [uid for uid in range(1, user_item.shape[0]) if uid != user_id]\n",
    "    similarity = []\n",
    "    num_interactions = []\n",
    "    \n",
    "    for uid in neighbor_id:\n",
    "        similarity.append(np.dot(user_item.loc[user_id], user_item.loc[uid]))\n",
    "        num_interactions.append(user_interactions.loc[uid])\n",
    "        \n",
    "    neighbors_df = pd.DataFrame({\n",
    "        'neighbor_id': neighbor_id,\n",
    "        'similarity': similarity,\n",
    "        'num_interactions': num_interactions\n",
    "    })\n",
    "    \n",
    "    neighbors_df.sort_values(['similarity', 'num_interactions'], ascending=False, inplace=True)\n",
    "    \n",
    "    return neighbors_df # Return the dataframe specified in the doc_string\n",
    "\n",
    "\n",
    "def user_user_recs_part2(user_id, m=10):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user id\n",
    "    m - (int) the number of recommendations you want for the user\n",
    "    \n",
    "    OUTPUT:\n",
    "    recs - (list) a list of recommendations for the user by article id\n",
    "    rec_names - (list) a list of recommendations for the user by article title\n",
    "    \n",
    "    Description:\n",
    "    Loops through the users based on closeness to the input user_id\n",
    "    For each user - finds articles the user hasn't seen before and provides them as recs\n",
    "    Does this until m recommendations are found\n",
    "    \n",
    "    Notes:\n",
    "    * Choose the users that have the most total article interactions \n",
    "    before choosing those with fewer article interactions.\n",
    "\n",
    "    * Choose articles with the articles with the most total interactions \n",
    "    before choosing those with fewer total interactions. \n",
    "   \n",
    "    '''\n",
    "    # Your code here\n",
    "    neighbors_df = get_top_sorted_users(user_id)\n",
    "    \n",
    "    top_m_neighbors = neighbors_df[:m]['neighbor_id']\n",
    "    \n",
    "    recs = []\n",
    "    for neighbor_id in top_m_neighbors:\n",
    "        article_ids = user_item.loc[neighbor_id]\n",
    "        recs.extend(str(aid) for aid in article_ids[article_ids == 1].index)\n",
    "        \n",
    "    recs = list(set(recs))[:m]  \n",
    "    rec_names = get_article_names(recs)\n",
    "    \n",
    "    return recs, rec_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 recommendations for user 20 are the following article ids:\n",
      "['125.0', '465.0', '1274.0', '681.0', '379.0', '1053.0', '1437.0', '337.0', '667.0', '1416.0']\n",
      "\n",
      "The top 10 recommendations for user 20 are the following article names:\n",
      "['data structures related to machine learning algorithms', 'access mysql with python', 'imitation learning in tensorflow (hopper from openai gym)', 'data model with streaming analytics and python', 'united states demographic measures: population and age', 'real-time sentiment analysis of twitter hashtags with spark (+ pixiedust)', 'introduction to neural networks, advantages and applications', 'what caused the challenger disaster?', 'statistics for hackers', 'generalization in deep learning']\n"
     ]
    }
   ],
   "source": [
    "# Quick spot check - don't change this code - just use it to test your functions\n",
    "rec_ids, rec_names = user_user_recs_part2(20, 10)\n",
    "print(\"The top 10 recommendations for user 20 are the following article ids:\")\n",
    "print(rec_ids)\n",
    "print()\n",
    "print(\"The top 10 recommendations for user 20 are the following article names:\")\n",
    "print(rec_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`5.` Use your functions from above to correctly fill in the solutions to the dictionary below.  Then test your dictionary against the solution.  Provide the code you need to answer each following the comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tests with a dictionary of results\n",
    "\n",
    "user1_most_sim = find_similar_users(1)[0]# Find the user that is most similar to user 1 \n",
    "user131_10th_sim = find_similar_users(131)[9]# Find the 10th most similar user to user 131"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3933"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user1_most_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user131_10th_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This all looks good!  Nice job!\n"
     ]
    }
   ],
   "source": [
    "## Dictionary Test Here\n",
    "sol_5_dict = {\n",
    "    'The user that is most similar to user 1.': user1_most_sim, \n",
    "    'The user that is the 10th most similar to user 131': user131_10th_sim,\n",
    "}\n",
    "\n",
    "t.sol_5_test(sol_5_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`6.` If we were given a new user, which of the above functions would you be able to use to make recommendations?  Explain.  Can you think of a better way we might make recommendations?  Use the cell below to explain a better method for new users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Provide your response here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`7.` Using your existing functions, provide the top 10 recommended articles you would provide for the a new user below.  You can test your function against our thoughts to make sure we are all on the same page with how we might make a recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_user = '0.0'\n",
    "\n",
    "# What would your recommendations be for this new user '0.0'?  As a new user, they have no observed articles.\n",
    "# Provide a list of the top 10 article ids you would give to \n",
    "new_user_recs = [str(id) for id in get_top_article_ids(10)]# Your recommendations here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's right!  Nice job!\n"
     ]
    }
   ],
   "source": [
    "assert set(new_user_recs) == set(['1314.0','1429.0','1293.0','1427.0','1162.0','1364.0','1304.0','1170.0','1431.0','1330.0']), \"Oops!  It makes sense that in this case we would want to recommend the most popular articles, because we don't know anything about these users.\"\n",
    "\n",
    "print(\"That's right!  Nice job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"Content-Recs\">Part IV: Content Based Recommendations (EXTRA - NOT REQUIRED)</a>\n",
    "\n",
    "Another method we might use to make recommendations is to perform a ranking of the highest ranked articles associated with some term.  You might consider content to be the **doc_body**, **doc_description**, or **doc_full_name**.  There isn't one way to create a content based recommendation, especially considering that each of these columns hold content related information.  \n",
    "\n",
    "`1.` Use the function body below to create a content based recommender.  Since there isn't one right answer for this recommendation tactic, no test functions are provided.  Feel free to change the function inputs if you decide you want to try a method that requires more input values.  The input values are currently set with one idea in mind that you may use to make content based recommendations.  One additional idea is that you might want to choose the most popular recommendations that meet your 'content criteria', but again, there is a lot of flexibility in how you might make these recommendations.\n",
    "\n",
    "### This part is NOT REQUIRED to pass this project.  However, you may choose to take this on as an extra way to show off your skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_content_recs():\n",
    "    '''\n",
    "    INPUT:\n",
    "    \n",
    "    OUTPUT:\n",
    "    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Now that you have put together your content-based recommendation system, use the cell below to write a summary explaining how your content based recommender works.  Do you see any possible improvements that could be made to your function?  Is there anything novel about your content based recommender?\n",
    "\n",
    "### This part is NOT REQUIRED to pass this project.  However, you may choose to take this on as an extra way to show off your skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write an explanation of your content based recommendation system here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Use your content-recommendation system to make recommendations for the below scenarios based on the comments.  Again no tests are provided here, because there isn't one right answer that could be used to find these content based recommendations.\n",
    "\n",
    "### This part is NOT REQUIRED to pass this project.  However, you may choose to take this on as an extra way to show off your skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make recommendations for a brand new user\n",
    "\n",
    "\n",
    "# make a recommendations for a user who only has interacted with article id '1427.0'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"Matrix-Fact\">Part V: Matrix Factorization</a>\n",
    "\n",
    "In this part of the notebook, you will build use matrix factorization to make article recommendations to the users on the IBM Watson Studio platform.\n",
    "\n",
    "`1.` You should have already created a **user_item** matrix above in **question 1** of **Part III** above.  This first question here will just require that you run the cells to get things set up for the rest of **Part V** of the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the matrix here\n",
    "user_item_matrix = pd.read_pickle('user_item_matrix.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick look at the matrix\n",
    "user_item_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` In this situation, you can use Singular Value Decomposition from [numpy](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.linalg.svd.html) on the user-item matrix.  Use the cell to perform SVD, and explain why this is different than in the lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform SVD on the User-Item Matrix Here\n",
    "\n",
    "u, s, vt = # use the built in to get the three matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Provide your response here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Now for the tricky part, how do we choose the number of latent features to use?  Running the below cell, you can see that as the number of latent features increases, we obtain a lower error rate on making predictions for the 1 and 0 values in the user-item matrix.  Run the cell below to get an idea of how the accuracy improves as we increase the number of latent features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_latent_feats = np.arange(10,700+10,20)\n",
    "sum_errs = []\n",
    "\n",
    "for k in num_latent_feats:\n",
    "    # restructure with k latent features\n",
    "    s_new, u_new, vt_new = np.diag(s[:k]), u[:, :k], vt[:k, :]\n",
    "    \n",
    "    # take dot product\n",
    "    user_item_est = np.around(np.dot(np.dot(u_new, s_new), vt_new))\n",
    "    \n",
    "    # compute error for each prediction to actual value\n",
    "    diffs = np.subtract(user_item_matrix, user_item_est)\n",
    "    \n",
    "    # total errors and keep track of them\n",
    "    err = np.sum(np.sum(np.abs(diffs)))\n",
    "    sum_errs.append(err)\n",
    "    \n",
    "    \n",
    "plt.plot(num_latent_feats, 1 - np.array(sum_errs)/df.shape[0]);\n",
    "plt.xlabel('Number of Latent Features');\n",
    "plt.ylabel('Accuracy');\n",
    "plt.title('Accuracy vs. Number of Latent Features');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` From the above, we can't really be sure how many features to use, because simply having a better way to predict the 1's and 0's of the matrix doesn't exactly give us an indication of if we are able to make good recommendations.  Instead, we might split our dataset into a training and test set of data, as shown in the cell below.  \n",
    "\n",
    "Use the code from question 3 to understand the impact on accuracy of the training and test sets of data with different numbers of latent features. Using the split below: \n",
    "\n",
    "* How many users can we make predictions for in the test set?  \n",
    "* How many users are we not able to make predictions for because of the cold start problem?\n",
    "* How many articles can we make predictions for in the test set?  \n",
    "* How many articles are we not able to make predictions for because of the cold start problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.head(40000)\n",
    "df_test = df.tail(5993)\n",
    "\n",
    "def create_test_and_train_user_item(df_train, df_test):\n",
    "    '''\n",
    "    INPUT:\n",
    "    df_train - training dataframe\n",
    "    df_test - test dataframe\n",
    "    \n",
    "    OUTPUT:\n",
    "    user_item_train - a user-item matrix of the training dataframe \n",
    "                      (unique users for each row and unique articles for each column)\n",
    "    user_item_test - a user-item matrix of the testing dataframe \n",
    "                    (unique users for each row and unique articles for each column)\n",
    "    test_idx - all of the test user ids\n",
    "    test_arts - all of the test article ids\n",
    "    \n",
    "    '''\n",
    "    # Your code here\n",
    "    \n",
    "    return user_item_train, user_item_test, test_idx, test_arts\n",
    "\n",
    "user_item_train, user_item_test, test_idx, test_arts = create_test_and_train_user_item(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the values in the dictionary below\n",
    "a = 662 \n",
    "b = 574 \n",
    "c = 20 \n",
    "d = 0 \n",
    "\n",
    "\n",
    "sol_4_dict = {\n",
    "    'How many users can we make predictions for in the test set?': # letter here, \n",
    "    'How many users in the test set are we not able to make predictions for because of the cold start problem?': # letter here, \n",
    "    'How many movies can we make predictions for in the test set?': # letter here,\n",
    "    'How many movies in the test set are we not able to make predictions for because of the cold start problem?': # letter here\n",
    "}\n",
    "\n",
    "t.sol_4_test(sol_4_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`5.` Now use the **user_item_train** dataset from above to find U, S, and V transpose using SVD. Then find the subset of rows in the **user_item_test** dataset that you can predict using this matrix decomposition with different numbers of latent features to see how many features makes sense to keep based on the accuracy on the test data. This will require combining what was done in questions `2` - `4`.\n",
    "\n",
    "Use the cells below to explore how well SVD works towards making predictions for recommendations on the test data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit SVD on the user_item_train matrix\n",
    "u_train, s_train, vt_train = # fit svd similar to above then use the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use these cells to see how well you can use the training \n",
    "# decomposition to predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "`6.` Use the cell below to comment on the results you found in the previous question. Given the circumstances of your results, discuss what you might do to determine if the recommendations you make with any of the above recommendation systems are an improvement to how users currently find articles? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your response here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='conclusions'></a>\n",
    "### Extras\n",
    "Using your workbook, you could now save your recommendations for each user, develop a class to make new predictions and update your results, and make a flask app to deploy your results.  These tasks are beyond what is required for this project.  However, from what you learned in the lessons, you certainly capable of taking these tasks on to improve upon your work here!\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "> Congratulations!  You have reached the end of the Recommendations with IBM project! \n",
    "\n",
    "> **Tip**: Once you are satisfied with your work here, check over your report to make sure that it is satisfies all the areas of the [rubric](https://review.udacity.com/#!/rubrics/2322/view). You should also probably remove all of the \"Tips\" like this one so that the presentation is as polished as possible.\n",
    "\n",
    "\n",
    "## Directions to Submit\n",
    "\n",
    "> Before you submit your project, you need to create a .html or .pdf version of this notebook in the workspace here. To do that, run the code cell below. If it worked correctly, you should get a return code of 0, and you should see the generated .html file in the workspace directory (click on the orange Jupyter icon in the upper left).\n",
    "\n",
    "> Alternatively, you can download this report as .html via the **File** > **Download as** submenu, and then manually upload it into the workspace directory by clicking on the orange Jupyter icon in the upper left, then using the Upload button.\n",
    "\n",
    "> Once you've done this, you can submit your project by clicking on the \"Submit Project\" button in the lower right here. This will create and submit a zip file with this .ipynb doc and the .html or .pdf version you created. Congratulations! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import call\n",
    "call(['python', '-m', 'nbconvert', 'Recommendations_with_IBM.ipynb'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
